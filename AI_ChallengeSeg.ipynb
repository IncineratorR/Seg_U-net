{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PKKD6hK26jPV"
      },
      "source": [
        "<center>\n",
        "<img src=\"https://i.imgur.com/DW7NRwf.png\" title=\"ElementalsAI\" width=80%/>\n",
        "</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CjwC8T6KuCCt"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "Deep Learning has been very successful in solving many complex problems in the real world. However, solving most of these challenges requires data. Additionally, having data is not just enough. The data has to be cleansed, annotated, and organized. Moreover, data in many domains is not accessible due to privacy constraints (Especially in the medical domain). Hence it becomes pertinent to create models that can learn from a limited amount of data.\n",
        "\n",
        "<br>\n",
        "\n",
        "## Problem Statement:\n",
        "In this task, you are given a small dataset of the microscopic view of the cells.\n",
        "\n",
        "*Your goal is to build a model that accurately predicts the cell regions as shown in the **Label Image**.*\n",
        "\n",
        "#### **Input Image**\n",
        "\n",
        "<a href=\"https://imgur.com/aLDNHwu\"><img src=\"https://i.imgur.com/aLDNHwu.png\" title=\"source: imgur.com\" /></a>\n",
        "\n",
        "#### **Label Image**\n",
        "\n",
        "<a href=\"https://imgur.com/s1mIkFE\"><img src=\"https://i.imgur.com/s1mIkFE.png\" title=\"source: imgur.com\" /></a>\n",
        "\n",
        "\n",
        "## Dataset:\n",
        "\n",
        "The dataset contains 30 training images along with the labels.\n",
        "\n",
        "[**Dataset Link**](https://drive.google.com/drive/folders/1678Tggykj46SpJZS9mKMKHw7YFmiGMc8?usp=sharing)\n",
        "\n",
        "\n",
        "## Evaluation\n",
        "\n",
        "Share the submission Jupyter/Colab notebook with the relevant explanation. code and models. We will evaluate your model on our **test dataset**.\n",
        "\n",
        "<br><br>\n",
        "****Note that this is an interview assignment & is not involved in the development of any software or solution.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qc6vK_ej5338"
      },
      "source": [
        "### Instructions\n",
        "\n",
        "1. Make a copy of this notebook to start editing & add your solution.\n",
        "2. A dataset folder has been shared with you to train and test.\n",
        "2. You can use any framework to develop the solution (Pytorch, Keras, Tensorflow, Theano, Caffe etc.).\n",
        "3. This assignment is a great medium to get to know you better. Please feel free to connect, interact & develop the solution. I would be more than willing to help you out in any issues or problems that you face while solving the challenge. You can connect with me at suraj.donthi@elementals.ai.\n",
        "4. The goal of this task is to understand how you approach solving a problem. The more you connect while developing the solution the better I will be able to understand you.\n",
        "5. Submission Files:\n",
        "    - Colab Notebook Link with Solutions Approach and Code Solution.\n",
        "    - Trained Model Link.\n",
        "    - Any other necessary files.\n",
        "5. **Submission deadline: Within 5 days of recieving the assignment, no later than 12 AM IST on the due date. The exact due date shall be mentioned in the email or the portal where you recieve the assignment.** (You can share the Colab Notebook to the gmail address surajdonthi.th@gmail.com)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gNmRTPRZ181E"
      },
      "source": [
        "## Explain the technique you will use to solve the problem in detail.\n",
        "\n",
        "- Include any model architectures, equations, diagrams etc. that is required to explain how you are going to solve the problem."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qPnOr-Nm5Wlv"
      },
      "source": [
        "To build a image segmetation model u need a relatively good dataset but the provided dataset contains only 30 images so i uses image segmentation to generate 100 images for training\n",
        "This model uses U-net architecture\n",
        "The U-Net model is a powerfull for image segmentation tasks. The model is relatively easy to train and can achieve state-of-the-art results on a variety of image segmentation datasets.\n",
        "This model is solely build by me so it has many problems , I have to fine tune it\n",
        "for now i trained the model\n",
        "This model contain 4 main scripts\n",
        "Fist one is for augmentation\n",
        "Second for preprocessing the images\n",
        "Third for converting the images and masks into .npy file\n",
        "Fourth is the training script itself```\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "================== `Your answer here. (Double click to edit)` =================="
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CfYy0MSR5Z7s"
      },
      "source": [
        "## Implement the solution below."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#AUGMENTATION The given dataset is too small to train a model for cell segmentaion thats why i use this script to generate 2000 images and masks with the given data\n"
      ],
      "metadata": {
        "id": "xicFpHVaXIXq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TUJjp7XTX2xB",
        "outputId": "56a839a5-b4a0-4ecb-f872-7ac17ee7d5e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "from skimage.transform import AffineTransform, warp\n",
        "from skimage import io, img_as_ubyte\n",
        "import random\n",
        "import os\n",
        "from scipy.ndimage import rotate\n",
        "\n",
        "import albumentations as A\n",
        "images_to_generate=1000\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "images_path=\"/content/gdrive/MyDrive/Projects/train_data/images\" #path to original images\n",
        "masks_path = \"/content/gdrive/MyDrive/Projects/train_data/masks\"\n",
        "img_augmented_path=\"/content/gdrive/MyDrive/Projects/aug/images/\" # path to store aumented images\n",
        "msk_augmented_path=\"/content/gdrive/MyDrive/Projects/aug/masks\" # path to store aumented images\n",
        "images=[] # to store paths of images from folder\n",
        "masks=[]\n",
        "\n",
        "for im in os.listdir(images_path):  # read image name from folder and append its path into \"images\" array\n",
        "    images.append(os.path.join(images_path,im))\n",
        "\n",
        "for msk in os.listdir(masks_path):  # read image name from folder and append its path into \"images\" array\n",
        "    masks.append(os.path.join(masks_path,msk))\n",
        "\n",
        "\n",
        "aug = A.Compose([\n",
        "    A.VerticalFlip(p=0.5),\n",
        "    A.RandomRotate90(p=0.5),\n",
        "    A.HorizontalFlip(p=1),\n",
        "    A.Transpose(p=1),\n",
        "    #A.ElasticTransform(p=1, alpha=120, sigma=120 * 0.05, alpha_affine=120 * 0.03),\n",
        "    A.GridDistortion(p=1)\n",
        "    ]\n",
        ")\n",
        "\n",
        "#random.seed(42)\n",
        "\n",
        "i=1   # variable to iterate till images_to_generate\n",
        "\n",
        "\n",
        "while i<=images_to_generate:\n",
        "    number = random.randint(0, len(images)-1)  #PIck a number to select an image & mask\n",
        "    image = images[number]\n",
        "    mask = masks[number]\n",
        "    print(image, mask)\n",
        "    #image=random.choice(images) #Randomly select an image name\n",
        "    original_image = io.imread(image)\n",
        "    original_mask = io.imread(mask)\n",
        "\n",
        "    augmented = aug(image=original_image, mask=original_mask)\n",
        "    transformed_image = augmented['image']\n",
        "    transformed_mask = augmented['mask']\n",
        "\n",
        "\n",
        "    new_image_path= \"%s/augmented_image_%s.tif\" %(img_augmented_path, i)\n",
        "    new_mask_path = \"%s/augmented_mask_%s.tif\" %(msk_augmented_path, i)\n",
        "    io.imsave(new_image_path, transformed_image)\n",
        "    io.imsave(new_mask_path, transformed_mask)\n",
        "    i =i+1"
      ],
      "metadata": {
        "id": "T-zr4CT1Xxs3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import tifffile\n",
        "import glob\n",
        "\n",
        "def load_image(path):\n",
        "  \"\"\"Load an image from a file.\n",
        "\n",
        "  Args:\n",
        "    path: The path to the image file.\n",
        "\n",
        "  Returns:\n",
        "    A PIL Image object.\n",
        "  \"\"\"\n",
        "\n",
        "  image = tifffile.imread(path)\n",
        "  image = Image.fromarray(image)\n",
        "\n",
        "  return image\n",
        "\n",
        "def load_mask(path):\n",
        "  \"\"\"Load a mask from a file.\n",
        "\n",
        "  Args:\n",
        "    path: The path to the mask file.\n",
        "\n",
        "  Returns:\n",
        "    A PIL Image object.\n",
        "  \"\"\"\n",
        "\n",
        "  mask = tifffile.imread(path)\n",
        "  mask = Image.fromarray(mask)\n",
        "\n",
        "  return mask\n",
        "\n",
        "def preprocess_image(image):\n",
        "  \"\"\"Preprocess an image for U-Net cell region prediction.\n",
        "\n",
        "  Args:\n",
        "    image: A PIL Image object.\n",
        "\n",
        "  Returns:\n",
        "    A preprocessed image tensor.\n",
        "  \"\"\"\n",
        "\n",
        "  # Resize the image to the desired size.\n",
        "  image = image.resize((256, 256))\n",
        "\n",
        "  # Normalize the pixel values to [0, 1].\n",
        "  image = np.asarray(image, dtype=np.float32) / 255.0\n",
        "\n",
        "  # Add a channel dimension.\n",
        "  image = image[..., np.newaxis]\n",
        "\n",
        "  return image\n",
        "\n",
        "def preprocess_mask(mask):\n",
        "  \"\"\"Preprocess a mask for U-Net cell region prediction.\n",
        "\n",
        "  Args:\n",
        "    mask: A PIL Image object.\n",
        "\n",
        "  Returns:\n",
        "    A preprocessed mask tensor.\n",
        "  \"\"\"\n",
        "\n",
        "  # Resize the mask to the desired size.\n",
        "  mask = mask.resize((256, 256))\n",
        "\n",
        "  # Convert the mask to a binary mask.\n",
        "  mask = np.asarray(mask, dtype=np.uint8)\n",
        "  mask = mask[..., 0]\n",
        "  mask = np.where(mask > 0, 1, 0)\n",
        "\n",
        "  # Add a channel dimension.\n",
        "  mask = mask[..., np.newaxis]\n",
        "\n",
        "  return mask\n",
        "\n",
        "def preprocess_dataset(image_paths, mask_paths):\n",
        "  \"\"\"Preprocess a dataset of images and masks for U-Net cell region prediction.\n",
        "\n",
        "  Args:\n",
        "    image_paths: A list of paths to image files.\n",
        "    mask_paths: A list of paths to mask files.\n",
        "\n",
        "  Returns:\n",
        "    A tuple of preprocessed image and mask tensors.\n",
        "  \"\"\"\n",
        "\n",
        "  preprocessed_images = []\n",
        "  preprocessed_masks = []\n",
        "\n",
        "  for image_path, mask_path in zip(image_paths, mask_paths):\n",
        "    image = load_image(image_path)\n",
        "    mask = load_mask(mask_path)\n",
        "\n",
        "    preprocessed_image = preprocess_image(image)\n",
        "    preprocessed_mask = preprocess_mask(mask)\n",
        "\n",
        "    preprocessed_images.append(preprocessed_image)\n",
        "    preprocessed_masks.append(preprocessed_mask)\n",
        "\n",
        "  return preprocessed_images, preprocessed_masks\n",
        "\n",
        "def main():\n",
        "  # Get the paths to the image and mask directories.\n",
        "  image_dir = '/content/gdrive/MyDrive/Projects/aug/images'\n",
        "  mask_dir = '/content/gdrive/MyDrive/Projects/aug/masks'\n",
        "\n",
        "  # Get the paths to all of the images and masks.\n",
        "  image_paths = glob.glob(os.path.join(image_dir, '*.tif'))\n",
        "  mask_paths = glob.glob(os.path.join(mask_dir, '*.tif'))\n",
        "\n",
        "  # Preprocess the images and masks.\n",
        "  preprocessed_images, preprocessed_masks = preprocess_dataset(image_paths, mask_paths)\n",
        "\n",
        "  # Train or predict the U-Net model using the preprocessed images and masks\n",
        "  # ...\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  main()"
      ],
      "metadata": {
        "id": "Aw0Ai5vCTEY0"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "def convert_to_npy(images, masks, output_dir):\n",
        "  \"\"\"Converts images and masks to NumPy arrays and saves them to a directory.\n",
        "\n",
        "  Args:\n",
        "    images: A list of paths to image files.\n",
        "    masks: A list of paths to mask files.\n",
        "    output_dir: The directory to save the NumPy arrays to.\n",
        "  \"\"\"\n",
        "\n",
        "  # Create the output directory if it does not exist.\n",
        "  if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "\n",
        "  # Convert the images and masks to NumPy arrays.\n",
        "  image_arrays = []\n",
        "  mask_arrays = []\n",
        "  for image_path, mask_path in zip(images, masks):\n",
        "    image = Image.open(image_path)\n",
        "    mask = Image.open(mask_path)\n",
        "\n",
        "    image_array = np.array(image)\n",
        "    mask_array = np.array(mask)\n",
        "\n",
        "    image_arrays.append(image_array)\n",
        "    mask_arrays.append(mask_array)\n",
        "\n",
        "  # Save the NumPy arrays to the output directory.\n",
        "  np.save(os.path.join(output_dir, 'images.npy'), image_arrays)\n",
        "  np.save(os.path.join(output_dir, 'masks.npy'), mask_arrays)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  # Get the paths to the image and mask directories.\n",
        "  image_dir = '/content/gdrive/MyDrive/Projects/aug/images'\n",
        "  mask_dir = '/content/gdrive/MyDrive/Projects/aug/masks'\n",
        "\n",
        "  # Get the paths to all of the images and masks.\n",
        "  image_paths = glob.glob(os.path.join(image_dir, '*.tif'))\n",
        "  mask_paths = glob.glob(os.path.join(mask_dir, '*.tif'))\n",
        "\n",
        "  # Convert the images and masks to NumPy arrays and save them to a directory.\n",
        "  output_dir = '/content/gdrive/MyDrive/Projects/aug/npy'\n",
        "  convert_to_npy(image_paths, mask_paths, output_dir)"
      ],
      "metadata": {
        "id": "EloGUOcRXrdw"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, UpSampling2D, Concatenate\n",
        "\n",
        "class UNet(tf.keras.Model):\n",
        "    def __init__(self):\n",
        "        super(UNet, self).__init__()\n",
        "\n",
        "        # Encoder\n",
        "        self.encoder = tf.keras.Sequential([\n",
        "            # Convert the input to a floating-point data type\n",
        "            tf.keras.layers.Lambda(lambda x: tf.cast(x, tf.float32)),\n",
        "\n",
        "            Conv2D(32, (3, 3), activation='relu', padding='same'),\n",
        "            MaxPooling2D((2, 2)),\n",
        "\n",
        "            Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
        "            MaxPooling2D((2, 2)),\n",
        "\n",
        "            Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
        "            MaxPooling2D((2, 2)),\n",
        "\n",
        "            Conv2D(256, (3, 3), activation='relu', padding='same'),\n",
        "            MaxPooling2D((2, 2)),\n",
        "        ])\n",
        "\n",
        "        # Decoder\n",
        "        self.decoder = tf.keras.Sequential([\n",
        "            UpSampling2D((2, 2)),\n",
        "            Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
        "\n",
        "            UpSampling2D((2, 2)),\n",
        "            Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
        "\n",
        "            UpSampling2D((2, 2)),\n",
        "            Conv2D(32, (3, 3), activation='relu', padding='same'),\n",
        "\n",
        "            UpSampling2D((2, 2)),\n",
        "            Conv2D(1, (1, 1), activation='sigmoid', padding='same'),\n",
        "        ])\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # Encode the input image\n",
        "        encoder_output = self.encoder(inputs)\n",
        "\n",
        "        # Decode the encoded features\n",
        "        decoder_output = self.decoder(encoder_output)\n",
        "\n",
        "        return decoder_output\n",
        "\n",
        "# Training script\n",
        "\n",
        "def train(model, train_images_path, train_masks_path, epochs=100):\n",
        "  \"\"\"Trains the U-Net model.\n",
        "\n",
        "  Args:\n",
        "    model: The U-Net model.\n",
        "    train_images_path: The path to the training images .npy file.\n",
        "    train_masks_path: The path to the training masks .npy file.\n",
        "    epochs: The number of epochs to train the model for.\n",
        "  \"\"\"\n",
        "\n",
        "  # Load the training images and masks\n",
        "  train_images = np.load(train_images_path)\n",
        "  train_masks = np.load(train_masks_path)\n",
        "\n",
        "  # Reshape the input images\n",
        "  train_images = train_images.reshape((-1, 512, 512, 1))\n",
        "  train_masks = train_masks.reshape((-1, 512, 512, 1))\n",
        "\n",
        "  # Build the model\n",
        "  model.call(train_images[:1])\n",
        "\n",
        "  # Compile the model\n",
        "  model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "  # Train the model\n",
        "  model.fit(train_images, train_masks, epochs=epochs)\n",
        "\n",
        "  # Save the model weights\n",
        "  model.save_weights('unet_model_weights.h5')\n",
        "\n",
        "# Example usage\n",
        "\n",
        "# Get the paths to the training images and masks .npy files\n",
        "train_images_path = '/content/gdrive/MyDrive/Projects/aug/npy/images.npy'\n",
        "train_masks_path = '/content/gdrive/MyDrive/Projects/aug/npy/masks.npy'\n",
        "\n",
        "# Create the U-Net model\n",
        "model = UNet()\n",
        "\n",
        "# Train the model\n",
        "train(model, train_images_path, train_masks_path)\n",
        "\n",
        "# Save the model weights\n",
        "model.save_weights('unet_model_weights.h5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wiNtVdia28bk",
        "outputId": "6881264f-4768-4337-90e8-8ed904e97b8c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "32/32 [==============================] - 17s 427ms/step - loss: -4051240960.0000 - accuracy: 0.0136\n",
            "Epoch 2/100\n",
            "32/32 [==============================] - 14s 432ms/step - loss: -13058077360128.0000 - accuracy: 0.0000e+00\n",
            "Epoch 3/100\n",
            "32/32 [==============================] - 14s 435ms/step - loss: -2105272440979456.0000 - accuracy: 0.0000e+00\n",
            "Epoch 4/100\n",
            "32/32 [==============================] - 14s 437ms/step - loss: -79876195453239296.0000 - accuracy: 0.0000e+00\n",
            "Epoch 5/100\n",
            "32/32 [==============================] - 14s 439ms/step - loss: -1277583995525136384.0000 - accuracy: 0.0000e+00\n",
            "Epoch 6/100\n",
            "32/32 [==============================] - 14s 441ms/step - loss: -11697305085187981312.0000 - accuracy: 0.0000e+00\n",
            "Epoch 7/100\n",
            "32/32 [==============================] - 14s 433ms/step - loss: nan - accuracy: 0.0726\n",
            "Epoch 8/100\n",
            "32/32 [==============================] - 13s 414ms/step - loss: nan - accuracy: 0.2195\n",
            "Epoch 9/100\n",
            "32/32 [==============================] - 13s 414ms/step - loss: nan - accuracy: 0.2195\n",
            "Epoch 10/100\n",
            "32/32 [==============================] - 13s 415ms/step - loss: nan - accuracy: 0.2195\n",
            "Epoch 11/100\n",
            "32/32 [==============================] - 13s 416ms/step - loss: nan - accuracy: 0.2195\n",
            "Epoch 12/100\n",
            "32/32 [==============================] - 13s 417ms/step - loss: nan - accuracy: 0.2195\n",
            "Epoch 13/100\n",
            "32/32 [==============================] - 13s 417ms/step - loss: nan - accuracy: 0.2195\n",
            "Epoch 14/100\n",
            "32/32 [==============================] - 13s 417ms/step - loss: nan - accuracy: 0.2195\n",
            "Epoch 15/100\n",
            "32/32 [==============================] - 13s 419ms/step - loss: nan - accuracy: 0.2195\n",
            "Epoch 16/100\n",
            "32/32 [==============================] - 13s 418ms/step - loss: nan - accuracy: 0.2195\n",
            "Epoch 17/100\n",
            "32/32 [==============================] - 13s 420ms/step - loss: nan - accuracy: 0.2195\n",
            "Epoch 18/100\n",
            "32/32 [==============================] - 13s 420ms/step - loss: nan - accuracy: 0.2195\n",
            "Epoch 19/100\n",
            "32/32 [==============================] - 13s 420ms/step - loss: nan - accuracy: 0.2195\n",
            "Epoch 20/100\n",
            "32/32 [==============================] - 13s 420ms/step - loss: nan - accuracy: 0.2195\n",
            "Epoch 21/100\n",
            "32/32 [==============================] - 13s 420ms/step - loss: nan - accuracy: 0.2195\n",
            "Epoch 22/100\n",
            "32/32 [==============================] - 13s 419ms/step - loss: nan - accuracy: 0.2195\n",
            "Epoch 23/100\n",
            "32/32 [==============================] - 13s 419ms/step - loss: nan - accuracy: 0.2195\n",
            "Epoch 24/100\n",
            "32/32 [==============================] - 13s 420ms/step - loss: nan - accuracy: 0.2195\n",
            "Epoch 25/100\n",
            "32/32 [==============================] - 13s 420ms/step - loss: nan - accuracy: 0.2195\n",
            "Epoch 26/100\n",
            "32/32 [==============================] - 13s 419ms/step - loss: nan - accuracy: 0.2195\n",
            "Epoch 27/100\n",
            "32/32 [==============================] - 13s 420ms/step - loss: nan - accuracy: 0.2195\n",
            "Epoch 28/100\n",
            "32/32 [==============================] - 13s 419ms/step - loss: nan - accuracy: 0.2195\n",
            "Epoch 29/100\n",
            "32/32 [==============================] - 13s 419ms/step - loss: nan - accuracy: 0.2195\n",
            "Epoch 30/100\n",
            "32/32 [==============================] - 13s 420ms/step - loss: nan - accuracy: 0.2195\n",
            "Epoch 31/100\n",
            "32/32 [==============================] - 13s 419ms/step - loss: nan - accuracy: 0.2195\n",
            "Epoch 32/100\n",
            "32/32 [==============================] - 13s 418ms/step - loss: nan - accuracy: 0.2195\n",
            "Epoch 33/100\n",
            "32/32 [==============================] - 13s 419ms/step - loss: nan - accuracy: 0.2195\n",
            "Epoch 34/100\n",
            "32/32 [==============================] - 13s 420ms/step - loss: nan - accuracy: 0.2195\n",
            "Epoch 35/100\n",
            "32/32 [==============================] - 13s 419ms/step - loss: nan - accuracy: 0.2195\n",
            "Epoch 36/100\n",
            "32/32 [==============================] - 13s 420ms/step - loss: nan - accuracy: 0.2195\n",
            "Epoch 37/100\n",
            "32/32 [==============================] - 13s 419ms/step - loss: nan - accuracy: 0.2195\n",
            "Epoch 38/100\n",
            "32/32 [==============================] - 13s 419ms/step - loss: nan - accuracy: 0.2195\n",
            "Epoch 39/100\n",
            "32/32 [==============================] - 13s 419ms/step - loss: nan - accuracy: 0.2195\n",
            "Epoch 40/100\n",
            "32/32 [==============================] - 13s 419ms/step - loss: nan - accuracy: 0.2195\n",
            "Epoch 41/100\n",
            "32/32 [==============================] - 13s 420ms/step - loss: nan - accuracy: 0.2195\n",
            "Epoch 42/100\n",
            "32/32 [==============================] - 13s 418ms/step - loss: nan - accuracy: 0.2195\n",
            "Epoch 43/100\n",
            "32/32 [==============================] - 13s 420ms/step - loss: nan - accuracy: 0.2195\n",
            "Epoch 44/100\n",
            "32/32 [==============================] - 13s 419ms/step - loss: nan - accuracy: 0.2195\n",
            "Epoch 45/100\n",
            "32/32 [==============================] - 13s 419ms/step - loss: nan - accuracy: 0.2195\n",
            "Epoch 46/100\n",
            "32/32 [==============================] - 13s 419ms/step - loss: nan - accuracy: 0.2195\n",
            "Epoch 47/100\n",
            "32/32 [==============================] - 13s 420ms/step - loss: nan - accuracy: 0.2195\n",
            "Epoch 48/100\n",
            "32/32 [==============================] - 13s 418ms/step - loss: nan - accuracy: 0.2195\n",
            "Epoch 49/100\n",
            "32/32 [==============================] - 13s 419ms/step - loss: nan - accuracy: 0.2195\n",
            "Epoch 50/100\n",
            "32/32 [==============================] - 13s 419ms/step - loss: nan - accuracy: 0.2195\n",
            "Epoch 51/100\n",
            "32/32 [==============================] - 13s 419ms/step - loss: nan - accuracy: 0.2195\n",
            "Epoch 52/100\n",
            "32/32 [==============================] - 13s 419ms/step - loss: nan - accuracy: 0.2195\n",
            "Epoch 53/100\n",
            "32/32 [==============================] - 13s 418ms/step - loss: nan - accuracy: 0.2195\n",
            "Epoch 54/100\n",
            "32/32 [==============================] - 13s 419ms/step - loss: nan - accuracy: 0.2195\n",
            "Epoch 55/100\n",
            "32/32 [==============================] - 13s 420ms/step - loss: nan - accuracy: 0.2195\n",
            "Epoch 56/100\n",
            "32/32 [==============================] - 13s 420ms/step - loss: nan - accuracy: 0.2195\n",
            "Epoch 57/100\n",
            "32/32 [==============================] - 13s 420ms/step - loss: nan - accuracy: 0.2195\n",
            "Epoch 58/100\n",
            "32/32 [==============================] - 13s 420ms/step - loss: nan - accuracy: 0.2195\n",
            "Epoch 59/100\n",
            "32/32 [==============================] - 13s 420ms/step - loss: nan - accuracy: 0.2195\n",
            "Epoch 60/100\n",
            "32/32 [==============================] - 13s 420ms/step - loss: nan - accuracy: 0.2195\n",
            "Epoch 61/100\n",
            "32/32 [==============================] - 13s 420ms/step - loss: nan - accuracy: 0.2195\n",
            "Epoch 62/100\n",
            "32/32 [==============================] - 13s 420ms/step - loss: nan - accuracy: 0.2195\n",
            "Epoch 63/100\n",
            "32/32 [==============================] - 13s 420ms/step - loss: nan - accuracy: 0.2195\n",
            "Epoch 64/100\n",
            "32/32 [==============================] - 13s 420ms/step - loss: nan - accuracy: 0.2195\n",
            "Epoch 65/100\n",
            "32/32 [==============================] - 13s 419ms/step - loss: nan - accuracy: 0.2195\n",
            "Epoch 66/100\n",
            "32/32 [==============================] - 13s 420ms/step - loss: nan - accuracy: 0.2195\n",
            "Epoch 67/100\n",
            "32/32 [==============================] - 13s 419ms/step - loss: nan - accuracy: 0.2195\n",
            "Epoch 68/100\n",
            "32/32 [==============================] - 13s 420ms/step - loss: nan - accuracy: 0.2195\n",
            "Epoch 69/100\n",
            "32/32 [==============================] - 13s 420ms/step - loss: nan - accuracy: 0.2195\n",
            "Epoch 70/100\n",
            "32/32 [==============================] - 13s 420ms/step - loss: nan - accuracy: 0.2195\n",
            "Epoch 71/100\n",
            "32/32 [==============================] - 13s 420ms/step - loss: nan - accuracy: 0.2195\n",
            "Epoch 72/100\n",
            "32/32 [==============================] - 13s 420ms/step - loss: nan - accuracy: 0.2195\n",
            "Epoch 73/100\n",
            "32/32 [==============================] - 13s 421ms/step - loss: nan - accuracy: 0.2195\n",
            "Epoch 74/100\n",
            "32/32 [==============================] - 13s 420ms/step - loss: nan - accuracy: 0.2195\n",
            "Epoch 75/100\n",
            "32/32 [==============================] - 13s 420ms/step - loss: nan - accuracy: 0.2195\n",
            "Epoch 76/100\n",
            "32/32 [==============================] - 13s 420ms/step - loss: nan - accuracy: 0.2195\n",
            "Epoch 77/100\n",
            "32/32 [==============================] - 13s 420ms/step - loss: nan - accuracy: 0.2195\n",
            "Epoch 78/100\n",
            "32/32 [==============================] - 13s 420ms/step - loss: nan - accuracy: 0.2195\n",
            "Epoch 79/100\n",
            "32/32 [==============================] - 13s 420ms/step - loss: nan - accuracy: 0.2195\n",
            "Epoch 80/100\n",
            "32/32 [==============================] - 13s 420ms/step - loss: nan - accuracy: 0.2195\n",
            "Epoch 81/100\n",
            "32/32 [==============================] - 13s 420ms/step - loss: nan - accuracy: 0.2195\n",
            "Epoch 82/100\n",
            "32/32 [==============================] - 13s 420ms/step - loss: nan - accuracy: 0.2195\n",
            "Epoch 83/100\n",
            "32/32 [==============================] - 13s 419ms/step - loss: nan - accuracy: 0.2195\n",
            "Epoch 84/100\n",
            "32/32 [==============================] - 13s 420ms/step - loss: nan - accuracy: 0.2195\n",
            "Epoch 85/100\n",
            "32/32 [==============================] - 13s 420ms/step - loss: nan - accuracy: 0.2195\n",
            "Epoch 86/100\n",
            "32/32 [==============================] - 13s 421ms/step - loss: nan - accuracy: 0.2195\n",
            "Epoch 87/100\n",
            "32/32 [==============================] - 13s 419ms/step - loss: nan - accuracy: 0.2195\n",
            "Epoch 88/100\n",
            "32/32 [==============================] - 13s 420ms/step - loss: nan - accuracy: 0.2195\n",
            "Epoch 89/100\n",
            "32/32 [==============================] - 13s 420ms/step - loss: nan - accuracy: 0.2195\n",
            "Epoch 90/100\n",
            "32/32 [==============================] - 13s 420ms/step - loss: nan - accuracy: 0.2195\n",
            "Epoch 91/100\n",
            "32/32 [==============================] - 13s 421ms/step - loss: nan - accuracy: 0.2195\n",
            "Epoch 92/100\n",
            "32/32 [==============================] - 13s 420ms/step - loss: nan - accuracy: 0.2195\n",
            "Epoch 93/100\n",
            "32/32 [==============================] - 13s 420ms/step - loss: nan - accuracy: 0.2195\n",
            "Epoch 94/100\n",
            "32/32 [==============================] - 13s 419ms/step - loss: nan - accuracy: 0.2195\n",
            "Epoch 95/100\n",
            "32/32 [==============================] - 13s 420ms/step - loss: nan - accuracy: 0.2195\n",
            "Epoch 96/100\n",
            "32/32 [==============================] - 13s 421ms/step - loss: nan - accuracy: 0.2195\n",
            "Epoch 97/100\n",
            "32/32 [==============================] - 13s 420ms/step - loss: nan - accuracy: 0.2195\n",
            "Epoch 98/100\n",
            "32/32 [==============================] - 13s 421ms/step - loss: nan - accuracy: 0.2195\n",
            "Epoch 99/100\n",
            "32/32 [==============================] - 13s 421ms/step - loss: nan - accuracy: 0.2195\n",
            "Epoch 100/100\n",
            "32/32 [==============================] - 13s 420ms/step - loss: nan - accuracy: 0.2195\n"
          ]
        }
      ]
    }
  ]
}